{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - Getting Started\n",
    "\n",
    "In several steps this tutorial will teach you everything you need to know to start tuning even the most complex kernels.\n",
    "\n",
    "If you are reading this tutorial on the Kernel Tuner's documentation pages, note that you can actually run this tutorial as a Jupyter Notebook. Just clone the Kernel Tuner's [GitHub repository](http://github.com/benvanwerkhoven/kernel_tuner). Install the Kernel Tuner using `pip install .` and Jupyter Notebooks using `pip install jupyter` and you're ready to go. You can start this tutorial by typing `jupyter notebook` in the `kernel_tuner/tutorial` directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've probably seen the rather minimalistic vector add examples from the [Kernel Tuner's Readme](https://github.com/benvanwerkhoven/kernel_tuner), which are \n",
    "a bit too simplistic to fully tell you how to tune any given kernel. \n",
    "Therefore, this tutorial starts out with a little bit more complex, yet \n",
    "still quite simple, 2D stencil kernel written in CUDA.\n",
    "\n",
    ">    If you prefer OpenCL over CUDA, don't worry. Everything in this tutorial \n",
    ">    applies as much to OpenCL as it does to CUDA. But I will use CUDA code in \n",
    ">    the examples, and CUDA terminology in the text. \n",
    ">    Here's a quick translation guide:  \n",
    ">    An OpenCL work_item is a called a thread in CUDA, a work_group is called a \n",
    ">    thread block, and an NDRange is called a grid. Instead of OpenCL's \n",
    ">    get_local_id() and get_group_id(), CUDA uses built-in variables threadIdx \n",
    ">    and blockIdx. \n",
    "\n",
    "## Tuning a 2D stencil kernel\n",
    "\n",
    "We use a 2D stencil kernel as an example kernel to get you started with writing\n",
    "your Python scripts to tune using the Kernel Tuner. 2D stencil kernels\n",
    "like the one we use here are an compute-intensive part of iterative solvers\n",
    "that are used by many applications that simulate physical processes, like\n",
    "diffusion. Let's say you have written a CUDA kernel to perform the 2D stencil\n",
    "computation on the GPU, like the one shown below.\n",
    "\n",
    "Like in any CUDA kernel, you as a programmer have to decide how to group your\n",
    "threads into thread blocks. And like in many CUDA kernels, the thread block\n",
    "size that we choose for our 2D stencil kernel is not really that important for\n",
    "the output of the kernel. However, the thread block dimensions will have an\n",
    "impact on the performance of your kernels. And the optimal setting will be\n",
    "different for different GPUs.\n",
    "\n",
    "So how do you know which thread block size to choose? Simply try them all with\n",
    "auto tuning!\n",
    "\n",
    "Make sure you execute the code block below, because we will need that later on, by selecting the cell and pressing **_shift+enter_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kernel_string = \"\"\"\n",
    "    #define domain_width    500\n",
    "    #define domain_height   500\n",
    "\n",
    "    __global__ void stencil_kernel(float *x_new, float *x_old) {\n",
    "        int x = blockIdx.x * block_size_x + threadIdx.x;\n",
    "        int y = blockIdx.y * block_size_y + threadIdx.y;\n",
    "\n",
    "        if (y>0 && y<domain_height-1 && x>0 && x<domain_width-1) {\n",
    "\n",
    "        x_new[y*domain_width+x] = ( x_old[ (y  ) * domain_width + (x  ) ] +\n",
    "                                    x_old[ (y  ) * domain_width + (x-1) ] +\n",
    "                                    x_old[ (y  ) * domain_width + (x+1) ] +\n",
    "                                    x_old[ (y+1) * domain_width + (x  ) ] +\n",
    "                                    x_old[ (y-1) * domain_width + (x  ) ] ) / 5.0f;\n",
    "\n",
    "        }\n",
    "    }\n",
    "\"\"\"\n",
    "print(\"ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This 2D stencil kernel assumes that a thread will be created for each element\n",
    "in the domain. Each thread then simply takes the average of the element\n",
    "corresponding with its computed thread index in ``x_old`` and its four direct\n",
    "neighbors, one in every direction. The newly computed value is then stored in\n",
    "``x_new``. Iterative solvers will have to call kernels like this one many\n",
    "times, so it is important that this kernel is efficient.\n",
    "\n",
    "You may notice that the kernel uses two, currently undefined, constants\n",
    "``block_size_x`` and ``block_size_y`` instead of built-in variables blockDim.x\n",
    "or blockDim.y. Setting the thread block dimensions at compile-time is often a\n",
    "good idea for performance. If you don't need to vary the thread block size at\n",
    "run-time, the compiler can, for example, unroll loops that iterate using the\n",
    "thread block size.\n",
    "\n",
    "Let's take a look at how we can write a small Python script that uses the\n",
    "Kernel Tuner to test the performance of our kernel for different combinations\n",
    "of ``block_size_x`` and ``block_size_y``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Setup tuning parameters\n",
    "\n",
    "We call parameters in the kernel, like ``block_size_x`` and ``block_size_y``,\n",
    "tunable parameters. This is because we want to tune the performance of the\n",
    "kernel based on the values given to these parameters.\n",
    "\n",
    "To tell the Kernel Tuner about our tunable parameters we use a Python\n",
    "dictionary, which is basically a hashmap. For every tunable parameter, we\n",
    "create a key-value pair in the dictionary. The key is the name of the parameter\n",
    "as a string. The value associated with that key is a list of possible values\n",
    "for the tunable parameter.\n",
    "\n",
    "Let's look at an example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "tune_params = OrderedDict()\n",
    "tune_params[\"block_size_x\"] = [32*i for i in range(1,9)]\n",
    "tune_params[\"block_size_y\"] = [2**i for i in range(6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just in case you are not a Python guru, an expression between square\n",
    "brackets ``[ ]`` is a list comprehension. ``[32*i for i in\n",
    "range(1,9)]`` will create a list of multiples of 32 ranging from\n",
    "``32*1`` up to ``32*8``. For ``block_size_y`` we creates a list of\n",
    "powers of 2 ranging from ``2**0 = 1`` up to ``2**5 = 32``.\n",
    "\n",
    "The values that we have picked here are just examples, you can basically\n",
    "pick any list of values that you like. The Kernel Tuner will check the\n",
    "maximum number of threads per thread block supported by your GPU at\n",
    "run-time, and automatically skip over kernel configurations that attempt\n",
    "to use more. The Kernel Tuner will do this silently, unless you use the\n",
    "option ``verbose=True``.\n",
    "\n",
    "While the Kernel Tuner allows you to pick any value that you like, an\n",
    "experienced CUDA programmer will know that only certain values will make sense.\n",
    "For example, a thread block size that is a multiple of 32 is likely to give\n",
    "better performance, because threads in CUDA are scheduled in warps of 32\n",
    "threads.\n",
    "\n",
    "For each kernel that the Kernel Tuner benchmarks, it will prepend the\n",
    "source code with C preprocessor directives to define all tuning\n",
    "parameters and their current value, for example:\n",
    "\n",
    "```\n",
    "    #define block_size_x 32\n",
    "    #define block_size_y 1\n",
    "```\n",
    "\n",
    "There is of course much more that you can tune within a kernel than just\n",
    "the thread block dimensions. Basically, you are completely free to write\n",
    "code that uses C preprocessor directives to change its behavior. If you\n",
    "tell the Kernel Tuner about all the possible values for this parameter, it\n",
    "will then benchmark all of possible execution paths in your\n",
    "code. However, the Kernel Tuner currently uses the convention that\n",
    "``block_size_x``, ``block_size_y``, and ``block_size_z`` are used for\n",
    "specifying the thread block dimensions.\n",
    "\n",
    "If you want to be able to compile your code when not using the Kernel\n",
    "Tuner, you can simply add preprocessor directives for providing default\n",
    "values to all tunable parameters, for example to the beginning our\n",
    "2D stencil kernel we could add:\n",
    "\n",
    "```\n",
    "    #ifndef block_size_x\n",
    "        #define block_size_x 16\n",
    "    #endif\n",
    "    #ifndef block_size_y\n",
    "        #define block_size_y 16\n",
    "    #endif\n",
    "```\n",
    "\n",
    "To ensure that the kernel code can be compiled directly by any CUDA compiler,\n",
    "even when the Kernel Tuner is not used.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling `tune_kernel()`\n",
    "\n",
    "Now that we've setup our tuning parameters it is time to look at how to call\n",
    "the Kernel Tuner, and most importantly ``tune_kernel()``.\n",
    "\n",
    "```python\n",
    "    tune_kernel(kernel_name, kernel_string, problem_size, arguments, tune_params,\n",
    "        grid_div_x=None, grid_div_y=None, restrictions=None, answer=None, \n",
    "        atol=1e-6, verbose=False, lang=None, device=0, platform=0, cmem_args=None,\n",
    "        sample=False, compiler_options=None, log=None)\n",
    "```\n",
    "\n",
    "As you can see, there are a lot optional parameters and we're not going\n",
    "to cover all of them right now, if you're interested check out the Kernel Tuner's [interface documentation](http://benvanwerkhoven.github.io/kernel_tuner/user-api.html).\n",
    "\n",
    "Let's start with the basics. The Kernel Tuner has to\n",
    "know at least the following things:\n",
    "\n",
    "* ``kernel_name``: The name of the kernel\n",
    "* ``kernel_string``: The source code that contains that kernel\n",
    "* ``problem_size``: The domain over which you create threads and thread blocks\n",
    "* ``arguments``: The arguments to use when calling the kernel\n",
    "* ``tune_params``: The dictionary with tunable parameters\n",
    "\n",
    "Note that, by executing the block at the top of this file we have already loaded the CUDA kernel code as a string and stored it as `kernel_string`.\n",
    "\n",
    "We can use Numpy to generate some random input data, and create a\n",
    "list of arguments that matches the argument list of our\n",
    "``stencil_kernel`` function written in CUDA. It is important that the\n",
    "order and type matches the function specification of our kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "problem_size = (500, 500)\n",
    "size = numpy.prod(problem_size)\n",
    "\n",
    "x_old = numpy.random.randn(size).astype(numpy.float32)\n",
    "x_new = numpy.copy(x_old)\n",
    "args = [x_new, x_old]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `size` matches the size of the domain in used in the CUDA code.\n",
    "Moreover, we use ``astype`` to ensure that the Numpy array consists of 32-bit floating-point values,\n",
    "as expected by our CUDA kernel.\n",
    "\n",
    "Instead of generating random data you can of course also use data from a\n",
    "file, Python offers many convenient functions for this, for example take\n",
    "look at numpy.fromfile() or numpy.loadtxt().\n",
    "\n",
    "The list named `args` will be used as the argument list that we'll pass\n",
    "to tune_kernel. The Kernel Tuner requires that list contains only Numpy arrays\n",
    "or Numpy scalar values, and that the order of arguments matches that of the CUDA kernel.\n",
    "\n",
    "Now we are almost ready to call tune_kernel(). However, we have not told\n",
    "the Kernel Tuner anything about how many thread blocks should be created\n",
    "to launch the kernel. If you do not specify this the Kernel Tuner will\n",
    "assume a default way for computing the number of thread blocks. The grid\n",
    "dimension in the x-direction will default to ``problem_size[0] / block_size_x`` and the y-direction will default to ``1``. This is\n",
    "convenient for small 1D kernels like vector add, but for our 2D stencil\n",
    "kernel we need to specify how the tuning parameters divide our problem\n",
    "size.\n",
    "\n",
    "You can tell the Kernel Tuner how to determine the number of blocks it\n",
    "should create through the so called grid divisor lists, which you can\n",
    "specify using the optional arguments ``grid_div_x`` and ``grid_div_y``.\n",
    "Now let's look at an example of how to setup these grid divisor lists.\n",
    "\n",
    "So for our 2D stencil kernel, we have a 2D domain over which we want to\n",
    "create threads and thread blocks in a way that we create one thread for\n",
    "each element in the domain. So to get to the number of thread blocks,\n",
    "the Kernel Tuner should just divide the problem_size in a particular\n",
    "dimension with the thread block size in that dimension. Therefore, we\n",
    "specify the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_div_x = [\"block_size_x\"]\n",
    "grid_div_y = [\"block_size_y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these are lists, you can add multiple tuning parameters to the\n",
    "list. If you want to, you can even write arithmetic expressions within\n",
    "these strings. The Kernel Tuner will evaluate all strings and multiply\n",
    "them together. Then it will use this product to divide the problem size\n",
    "in that dimension **rounded up**.\n",
    "\n",
    "### Putting it all together\n",
    "\n",
    "Now let's put everything that we've gone through together in a Python script\n",
    "so you can try it out and see what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import kernel_tuner\n",
    "\n",
    "results = kernel_tuner.tune_kernel(\"stencil_kernel\", kernel_string, problem_size,\n",
    "        args, tune_params, grid_div_x=grid_div_x, grid_div_y=grid_div_y,\n",
    "        verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call `tune_kernel()` the Kernel Tuner begins with printing the name of the GPU it is using, just to be sure which GPU is used when you have multiple GPUs in your system. \n",
    "\n",
    "After that it will iterative compile and benchmark the kernel for every possible combination of all values of all tunable parameters. As you will see in the output, the Kernel Tuner will automatically skip over kernels that use too many threads per block. It will do that too for kernels that use too much shared memory or too many registers. These configurations are by default skipped silently, but are currently being printed because we have passed the `verbose=True` option.\n",
    "\n",
    "As a convenience, the Kernel Tuner also prints the best performing combination of tunable parameters. It is however, more useful to be able to load the tuning results as data.\n",
    "\n",
    "### Storing the results\n",
    "\n",
    "While it's nice that the Kernel Tuner prints the results to stdout, it's not that great if we had to parse what is printed to get the results. That is why the `tune_kernel()` returns a data structure that holds all the results.\n",
    "\n",
    "`tune_kernel` returns a list of dictionaries, where each benchmarked kernel is represented by a dictionary contain the tunable parameters for that particular kernel configuration and one more entry called 'time'. The list of dictionaries format is very flexible and can easily be converted to easy to parse formats like json or csv for further analysis.\n",
    "\n",
    "You can execute the following code block to store the tuning results to both a json and a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#store output as json\n",
    "import json\n",
    "with open(\"tutorial.json\", 'w') as fp:\n",
    "    json.dump(results, fp)\n",
    "\n",
    "#store output as csv\n",
    "from pandas import DataFrame\n",
    "df = DataFrame(results)\n",
    "df.to_csv(\"tutorial.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
