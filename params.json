{
  "name": "Kernel tuner",
  "tagline": "A simple CUDA kernel tuner in Python",
  "body": "\r\nA simple CUDA kernel tuner in Python\r\n====================================\r\n[![Build Status](https://api.travis-ci.org/benvanwerkhoven/kernel_tuner.svg?branch=master)](https://travis-ci.org/benvanwerkhoven/kernel_tuner)\r\n[![Codacy Badge](https://api.codacy.com/project/badge/grade/016dc85044ab4d57b777449d93275608)](https://www.codacy.com/app/b-vanwerkhoven/kernel_tuner)\r\n\r\nThe goal of this project is to provide a - as simple as possible - tool \r\nfor tuning CUDA and OpenCL kernels. This implies that any CUDA or OpenCL \r\nkernel can be tuned without requiring extensive changes to the original \r\nkernel code.\r\n\r\nA very common problem in GPU programming is that some combination of \r\nthread block dimensions and other kernel parameters, like tiling or \r\nunrolling factors, results in dramatically better performance than other \r\nkernel configurations. The goal of auto-tuning is to automate the \r\nprocess of finding the best performing configuration for a given device.\r\n\r\nThis kernel tuner aims that you can directly use the tuned kernel\r\nwithout introducing any new dependencies. The tuned kernels can\r\nafterwards be used independently of the programming environment, whether\r\nthat is using C/C++/Java/Fortran or Python doesn't matter.\r\n\r\nThe kernel_tuner module currently only contains one function which is called\r\ntune_kernel to which you pass at least the kernel name, a string\r\ncontaining the kernel code, the problem size, a list of kernel function\r\narguments, and a dictionary of tunable parameters. There are also a lot\r\nof optional parameters, for a full list see the documentation of\r\ntune_kernel.\r\n\r\nDocumentation\r\n-------------\r\nThe full documentation is available [here](http://benvanwerkhoven.github.io/kernel_tuner/sphinxdoc/html/index.html).\r\n\r\nInstallation\r\n------------\r\nclone the repository  \r\n    `git clone git@github.com:benvanwerkhoven/kernel_tuner.git`  \r\nchange into the top-level directory  \r\n    `cd kernel_tuner`  \r\ninstall using  \r\n    `pip install .`\r\n\r\nDependencies\r\n------------\r\n * PyCuda and/or PyOpenCL (https://mathema.tician.de/software/)\r\n\r\nExample usage\r\n-------------\r\nThe following shows a simple example use of the kernel tuner:\r\n\r\n```python\r\n    kernel_string = \"\"\"\r\n    __global__ void vector_add(float *c, float *a, float *b, int n) {\r\n        int i = blockIdx.x * block_size_x + threadIdx.x;\r\n        if (i<n) {\r\n            c[i] = a[i] + b[i];\r\n        }\r\n    }\r\n    \"\"\"\r\n\r\n    size = 10000000\r\n    problem_size = (size, 1)\r\n\r\n    a = numpy.random.randn(size).astype(numpy.float32)\r\n    b = numpy.random.randn(size).astype(numpy.float32)\r\n    c = numpy.zeros_like(b)\r\n    n = numpy.int32(size)\r\n    args = [c, a, b, n]\r\n\r\n    tune_params = dict()\r\n    tune_params[\"block_size_x\"] = [128+64*i for i in range(15)]\r\n\r\n    tune_kernel(\"vector_add\", kernel_string, problem_size, args, tune_params)\r\n```\r\nAnd for OpenCL:\r\n```python\r\n    kernel_string = \"\"\"\r\n    __kernel void vector_add(__global float *c, __global float *a, __global float *b, int n) {\r\n        int i = get_global_id(0);\r\n        if (i<n) {\r\n            c[i] = a[i] + b[i];\r\n        }\r\n    }\r\n    \"\"\"\r\n\r\n    size = 10000000\r\n    problem_size = (size, 1)\r\n\r\n    a = numpy.random.rand(size).astype(numpy.float32)\r\n    b = numpy.random.rand(size).astype(numpy.float32)\r\n    c = numpy.zeros_like(a)\r\n    n = numpy.int32(size)\r\n\r\n    args = [c, a, b, n]\r\n\r\n    tune_params = dict()\r\n    tune_params[\"block_size_x\"] = [128+64*i for i in range(15)]\r\n\r\n    tune_kernel(\"vector_add\", kernel_string, problem_size, args, tune_params)\r\n\r\n```\r\nMore extensive examples are available in the `examples` directory\r\n\r\nContribution guide\r\n------------------\r\nThe kernel tuner follows the Google Python style guide. If you want to\r\ncontribute to the project please fork it, create a branch including\r\nyour addition, and create a pull request.\r\n\r\nContributing authors so far:\r\n* Ben van Werkhoven\r\n\r\n\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}