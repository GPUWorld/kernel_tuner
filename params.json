{
  "name": "Kernel tuner",
  "tagline": "A simple CUDA kernel tuner in Python",
  "body": "\r\nA simple CUDA/OpenCL kernel tuner in Python\r\n====================================\r\n[![Build Status](https://api.travis-ci.org/benvanwerkhoven/kernel_tuner.svg?branch=master)](https://travis-ci.org/benvanwerkhoven/kernel_tuner)\r\n[![Codacy Badge](https://api.codacy.com/project/badge/grade/016dc85044ab4d57b777449d93275608)](https://www.codacy.com/app/b-vanwerkhoven/kernel_tuner)\r\n[![Codacy Badge](https://api.codacy.com/project/badge/coverage/016dc85044ab4d57b777449d93275608)](https://www.codacy.com/app/b-vanwerkhoven/kernel_tuner)\r\n\r\nThe goal of this project is to provide a - as simple as possible - tool \r\nfor tuning CUDA and OpenCL kernels. This implies that any CUDA or OpenCL \r\nkernel can be tuned without requiring extensive changes to the original \r\nkernel code.\r\n\r\nA very common problem in GPU programming is that some combination of \r\nthread block dimensions and other kernel parameters, like tiling or \r\nunrolling factors, results in dramatically better performance than other \r\nkernel configurations. The goal of auto-tuning is to automate the \r\nprocess of finding the best performing configuration for a given device.\r\n\r\nThis kernel tuner aims that you can directly use the tuned kernel\r\nwithout introducing any new dependencies. The tuned kernels can\r\nafterwards be used independently of the programming environment, whether\r\nthat is using C/C++/Java/Fortran or Python doesn't matter.\r\n\r\nThe kernel_tuner module currently only contains main one function which\r\nis called tune_kernel to which you pass at least the kernel name, a string\r\ncontaining the kernel code, the problem size, a list of kernel function\r\narguments, and a dictionary of tunable parameters. There are also a lot\r\nof optional parameters, for a complete list see the full documentation of\r\n[tune_kernel](http://benvanwerkhoven.github.io/kernel_tuner/sphinxdoc/html/details.html).\r\n\r\nDocumentation\r\n-------------\r\nThe full documentation is available [here](http://benvanwerkhoven.github.io/kernel_tuner/sphinxdoc/html/index.html).\r\n\r\nInstallation\r\n------------\r\nclone the repository  \r\n    `git clone git@github.com:benvanwerkhoven/kernel_tuner.git`  \r\nchange into the top-level directory  \r\n    `cd kernel_tuner`  \r\ninstall using  \r\n    `pip install .`\r\n\r\nDependencies\r\n------------\r\n * Python 2.7 or Python 3.5\r\n * PyCuda and/or PyOpenCL (https://mathema.tician.de/software/)\r\n\r\nExample usage\r\n-------------\r\nThe following shows a simple example for tuning a CUDA kernel:\r\n\r\n```python\r\nkernel_string = \"\"\"\r\n__global__ void vector_add(float *c, float *a, float *b, int n) {\r\n    int i = blockIdx.x * block_size_x + threadIdx.x;\r\n    if (i<n) {\r\n        c[i] = a[i] + b[i];\r\n    }\r\n}\r\n\"\"\"\r\n\r\nsize = 10000000\r\nproblem_size = (size, 1)\r\n\r\na = numpy.random.randn(size).astype(numpy.float32)\r\nb = numpy.random.randn(size).astype(numpy.float32)\r\nc = numpy.zeros_like(b)\r\nn = numpy.int32(size)\r\nargs = [c, a, b, n]\r\n\r\ntune_params = dict()\r\ntune_params[\"block_size_x\"] = [128+64*i for i in range(15)]\r\n\r\ntune_kernel(\"vector_add\", kernel_string, problem_size, args, tune_params)\r\n```\r\nThe exact same Python code can be used to tune an OpenCL kernel:\r\n```python\r\nkernel_string = \"\"\"\r\n__kernel void vector_add(__global float *c, __global float *a, __global float *b, int n) {\r\n    int i = get_global_id(0);\r\n    if (i<n) {\r\n        c[i] = a[i] + b[i];\r\n    }\r\n}\r\n\"\"\"\r\n```\r\nOr even just a C function, with slightly different tunable parameters:\r\n```python\r\ntune_params = dict()\r\ntune_params[\"vecsize\"] = [2**i for i in range(8)]\r\ntune_params[\"nthreads\"] = [1, 2, 3, 4, 6, 8, 12, 16, 24, 32]\r\n\r\nkernel_string = \"\"\" \r\n#include <omp.h>\r\n#include \"timer.h\"\r\ntypedef float vfloat __attribute__ ((vector_size (vecsize*4)));\r\nfloat vector_add(vfloat *c, vfloat *a, vfloat *b, int n) {\r\n    unsigned long long start = get_clock();\r\n    int chunk = n/vecsize/nthreads;\r\n    #pragma omp parallel num_threads(nthreads)\r\n    {\r\n       \tint offset = omp_get_thread_num()*chunk;\r\n       \tfor (int i = offset; i<offset+chunk; i++) {\r\n            c[i] = a[i] + b[i];\r\n       \t}\r\n    }\r\n    return (get_clock()-start) / get_frequency() / 1000000.0;\r\n}\r\n\"\"\"\r\n```\r\nBy passing an `answer` list you can let de kernel tuner verify the output of each kernel it compiles and benchmarks:\r\n```python\r\nanswer = [a+b, None, None]  # the order matches the arguments (in args) to the kernel\r\ntune_kernel(\"vector_add\", kernel_string, problem_size, args, tune_params, answer=answer)\r\n```\r\nYou can find these and many - more extensive - example codes, in the `examples` directory\r\nand in the [full documentation](http://benvanwerkhoven.github.io/kernel_tuner/sphinxdoc/html/index.html).\r\n\r\nContribution guide\r\n------------------\r\nThe kernel tuner follows the Google Python style guide, with Sphinxdoc docstrings for module public functions. If you want to\r\ncontribute to the project please fork it, create a branch including your addition, and create a pull request.\r\n\r\nThe tests use relative imports and can be run directly after making\r\nchanges to the code. To run all tests use `nosetests` in the main directory.\r\nTo run the examples after code changes, you need to run `pip install --upgrade .` in the main directory.\r\nDocumentation is generated by typing `make html` in the doc directory, the contents\r\nof `doc/build/html/` should then be copied to `sphinxdoc` directory of the `gh-pages` branch.\r\n\r\nBefore creating a pull request please ensure the following:\r\n* You have written unit tests to test your additions\r\n* All unit tests pass\r\n* The examples still work and produce the same (or better) results\r\n* The code is compatible with both Python 2.7 and Python 3.5\r\n* An entry about the change or addition is created in CHANGELOG.md\r\n\r\nContributing authors so far:\r\n* Ben van Werkhoven\r\n* Berend Weel\r\n\r\nRelated work\r\n------------\r\nYou may also like [CLTune](https://github.com/CNugteren/CLTune) by Cedric\r\nNugteren. CLTune is a C++ library for kernel tuning and supports various\r\nadvanced features like machine learning to optimize the time spent on tuning\r\nkernels.\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}