

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Tutorial - Getting Started &mdash; kernel_tuner 0.1.1 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="kernel_tuner 0.1.1 documentation" href="index.html"/>
        <link rel="next" title="Convolution Example" href="convolution.html"/>
        <link rel="prev" title="Kernel Tuner Examples" href="examples.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> kernel_tuner
          

          
          </a>

          
            
            
              <div class="version">
                0.1.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Kernel Tuner Examples</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tutorial - Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Tuning-a-2D-stencil-kernel">Tuning a 2D stencil kernel</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Setup-tuning-parameters">Setup tuning parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Calling-tune_kernel()">Calling <code class="docutils literal"><span class="pre">tune_kernel()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#Putting-it-all-together">Putting it all together</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Storing-the-results">Storing the results</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="convolution.html">Convolution Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix.html">Matrix Multiply Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="correctness.html">Kernel Correctness Verification</a></li>
<li class="toctree-l1"><a class="reference internal" href="hostcode.html">Tuning Host Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="user-api.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="design.html">Design documentation</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">kernel_tuner</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>Tutorial - Getting Started</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/tutorial.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="Tutorial---Getting-Started">
<h1>Tutorial - Getting Started<a class="headerlink" href="#Tutorial---Getting-Started" title="Permalink to this headline">Â¶</a></h1>
<p>In several steps this tutorial will teach you everything you need to
know to start tuning even the most complex kernels.</p>
<p>If you are reading this tutorial on the Kernel Tuner&#8217;s documentation
pages, note that you can actually run this tutorial as a Jupyter
Notebook. Just clone the Kernel Tuner&#8217;s <a class="reference external" href="http://github.com/benvanwerkhoven/kernel_tuner">GitHub
repository</a>. Install
the Kernel Tuner using <code class="docutils literal"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">.</span></code> and Jupyter Notebooks using
<code class="docutils literal"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">jupyter</span></code> and you&#8217;re ready to go. You can start this
tutorial by typing <code class="docutils literal"><span class="pre">jupyter</span> <span class="pre">notebook</span></code> in the <code class="docutils literal"><span class="pre">kernel_tuner/tutorial</span></code>
directory.</p>
<p>You&#8217;ve probably seen the rather minimalistic vector add examples from
the <a class="reference external" href="https://github.com/benvanwerkhoven/kernel_tuner">Kernel Tuner&#8217;s
Readme</a>, which are a
bit too simplistic to fully tell you how to tune any given kernel.
Therefore, this tutorial starts out with a little bit more complex, yet
still quite simple, 2D stencil kernel written in CUDA.</p>
<blockquote>
<div><div class="line-block">
<div class="line">If you prefer OpenCL over CUDA, don&#8217;t worry. Everything in this
tutorial applies as much to OpenCL as it does to CUDA. But I will
use CUDA code in the examples, and CUDA terminology in the text.
Here&#8217;s a quick translation guide:</div>
<div class="line">An OpenCL work_item is a called a thread in CUDA, a work_group
is called a thread block, and an NDRange is called a grid. Instead
of OpenCL&#8217;s get_local_id() and get_group_id(), CUDA uses
built-in variables threadIdx and blockIdx.</div>
</div>
</div></blockquote>
<div class="section" id="Tuning-a-2D-stencil-kernel">
<h2>Tuning a 2D stencil kernel<a class="headerlink" href="#Tuning-a-2D-stencil-kernel" title="Permalink to this headline">Â¶</a></h2>
<p>We use a 2D stencil kernel as an example kernel to get you started with
writing your Python scripts to tune using the Kernel Tuner. 2D stencil
kernels like the one we use here are an compute-intensive part of
iterative solvers that are used by many applications that simulate
physical processes, like diffusion. Let&#8217;s say you have written a CUDA
kernel to perform the 2D stencil computation on the GPU, like the one
shown below.</p>
<p>Like in any CUDA kernel, you as a programmer have to decide how to group
your threads into thread blocks. And like in many CUDA kernels, the
thread block size that we choose for our 2D stencil kernel is not really
that important for the output of the kernel. However, the thread block
dimensions will have an impact on the performance of your kernels. And
the optimal setting will be different for different GPUs.</p>
<p>So how do you know which thread block size to choose? Simply try them
all with auto tuning!</p>
<p>Make sure you execute the code block below, because we will need that
later on, by selecting the cell and pressing <strong>*shift+enter*</strong>.</p>
<div class="nbinput nblast container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">kernel_string</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    #define domain_width    500</span>
<span class="s2">    #define domain_height   500</span>

<span class="s2">    __global__ void stencil_kernel(float *x_new, float *x_old) {</span>
<span class="s2">        int x = blockIdx.x * block_size_x + threadIdx.x;</span>
<span class="s2">        int y = blockIdx.y * block_size_y + threadIdx.y;</span>

<span class="s2">        if (y&gt;0 &amp;&amp; y&lt;domain_height-1 &amp;&amp; x&gt;0 &amp;&amp; x&lt;domain_width-1) {</span>

<span class="s2">        x_new[y*domain_width+x] = ( x_old[ (y  ) * domain_width + (x  ) ] +</span>
<span class="s2">                                    x_old[ (y  ) * domain_width + (x-1) ] +</span>
<span class="s2">                                    x_old[ (y  ) * domain_width + (x+1) ] +</span>
<span class="s2">                                    x_old[ (y+1) * domain_width + (x  ) ] +</span>
<span class="s2">                                    x_old[ (y-1) * domain_width + (x  ) ] ) / 5.0f;</span>

<span class="s2">        }</span>
<span class="s2">    }</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ok!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>This 2D stencil kernel assumes that a thread will be created for each
element in the domain. Each thread then simply takes the average of the
element corresponding with its computed thread index in <code class="docutils literal"><span class="pre">x_old</span></code> and
its four direct neighbors, one in every direction. The newly computed
value is then stored in <code class="docutils literal"><span class="pre">x_new</span></code>. Iterative solvers will have to call
kernels like this one many times, so it is important that this kernel is
efficient.</p>
<p>You may notice that the kernel uses two, currently undefined, constants
<code class="docutils literal"><span class="pre">block_size_x</span></code> and <code class="docutils literal"><span class="pre">block_size_y</span></code> instead of built-in variables
blockDim.x or blockDim.y. Setting the thread block dimensions at
compile-time is often a good idea for performance. If you don&#8217;t need to
vary the thread block size at run-time, the compiler can, for example,
unroll loops that iterate using the thread block size.</p>
<p>Let&#8217;s take a look at how we can write a small Python script that uses
the Kernel Tuner to test the performance of our kernel for different
combinations of <code class="docutils literal"><span class="pre">block_size_x</span></code> and <code class="docutils literal"><span class="pre">block_size_y</span></code>.</p>
<div class="section" id="Setup-tuning-parameters">
<h3>Setup tuning parameters<a class="headerlink" href="#Setup-tuning-parameters" title="Permalink to this headline">Â¶</a></h3>
<p>We call parameters in the kernel, like <code class="docutils literal"><span class="pre">block_size_x</span></code> and
<code class="docutils literal"><span class="pre">block_size_y</span></code>, tunable parameters. This is because we want to tune
the performance of the kernel based on the values given to these
parameters.</p>
<p>To tell the Kernel Tuner about our tunable parameters we use a Python
dictionary, which is basically a hashmap. For every tunable parameter,
we create a key-value pair in the dictionary. The key is the name of the
parameter as a string. The value associated with that key is a list of
possible values for the tunable parameter.</p>
<p>Let&#8217;s look at an example:</p>
<div class="nbinput nblast container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="k">import</span> <span class="n">OrderedDict</span>
<span class="n">tune_params</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
<span class="n">tune_params</span><span class="p">[</span><span class="s2">&quot;block_size_x&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">32</span><span class="o">*</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">9</span><span class="p">)]</span>
<span class="n">tune_params</span><span class="p">[</span><span class="s2">&quot;block_size_y&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<p>Now just in case you are not a Python guru, an expression between square
brackets <code class="docutils literal"><span class="pre">[</span> <span class="pre">]</span></code> is a list comprehension. <code class="docutils literal"><span class="pre">[32*i</span> <span class="pre">for</span> <span class="pre">i</span> <span class="pre">in</span> <span class="pre">range(1,9)]</span></code>
will create a list of multiples of 32 ranging from <code class="docutils literal"><span class="pre">32*1</span></code> up to
<code class="docutils literal"><span class="pre">32*8</span></code>. For <code class="docutils literal"><span class="pre">block_size_y</span></code> we creates a list of powers of 2 ranging
from <code class="docutils literal"><span class="pre">2**0</span> <span class="pre">=</span> <span class="pre">1</span></code> up to <code class="docutils literal"><span class="pre">2**5</span> <span class="pre">=</span> <span class="pre">32</span></code>.</p>
<p>The values that we have picked here are just examples, you can basically
pick any list of values that you like. The Kernel Tuner will check the
maximum number of threads per thread block supported by your GPU at
run-time, and automatically skip over kernel configurations that attempt
to use more. The Kernel Tuner will do this silently, unless you use the
option <code class="docutils literal"><span class="pre">verbose=True</span></code>.</p>
<p>While the Kernel Tuner allows you to pick any value that you like, an
experienced CUDA programmer will know that only certain values will make
sense. For example, a thread block size that is a multiple of 32 is
likely to give better performance, because threads in CUDA are scheduled
in warps of 32 threads.</p>
<p>For each kernel that the Kernel Tuner benchmarks, it will prepend the
source code with C preprocessor directives to define all tuning
parameters and their current value, for example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1">#define block_size_x 32</span>
<span class="c1">#define block_size_y 1</span>
</pre></div>
</div>
<p>There is of course much more that you can tune within a kernel than just
the thread block dimensions. Basically, you are completely free to write
code that uses C preprocessor directives to change its behavior. If you
tell the Kernel Tuner about all the possible values for this parameter,
it will then benchmark all of possible execution paths in your code.
However, the Kernel Tuner currently uses the convention that
<code class="docutils literal"><span class="pre">block_size_x</span></code>, <code class="docutils literal"><span class="pre">block_size_y</span></code>, and <code class="docutils literal"><span class="pre">block_size_z</span></code> are used for
specifying the thread block dimensions.</p>
<p>If you want to be able to compile your code when not using the Kernel
Tuner, you can simply add preprocessor directives for providing default
values to all tunable parameters, for example to the beginning our 2D
stencil kernel we could add:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1">#ifndef block_size_x</span>
    <span class="c1">#define block_size_x 16</span>
<span class="c1">#endif</span>
<span class="c1">#ifndef block_size_y</span>
    <span class="c1">#define block_size_y 16</span>
<span class="c1">#endif</span>
</pre></div>
</div>
<p>To ensure that the kernel code can be compiled directly by any CUDA
compiler, even when the Kernel Tuner is not used.</p>
</div>
<div class="section" id="Calling-tune_kernel()">
<h3>Calling <code class="docutils literal"><span class="pre">tune_kernel()</span></code><a class="headerlink" href="#Calling-tune_kernel()" title="Permalink to this headline">Â¶</a></h3>
<p>Now that we&#8217;ve setup our tuning parameters it is time to look at how to
call the Kernel Tuner, and most importantly <code class="docutils literal"><span class="pre">tune_kernel()</span></code>.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">tune_kernel</span><span class="p">(</span><span class="n">kernel_name</span><span class="p">,</span> <span class="n">kernel_string</span><span class="p">,</span> <span class="n">problem_size</span><span class="p">,</span> <span class="n">arguments</span><span class="p">,</span> <span class="n">tune_params</span><span class="p">,</span>
    <span class="n">grid_div_x</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">grid_div_y</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">restrictions</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">answer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">atol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">platform</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cmem_args</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">sample</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">compiler_options</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>
</div>
<p>As you can see, there are a lot optional parameters and we&#8217;re not going
to cover all of them right now, if you&#8217;re interested check out the
Kernel Tuner&#8217;s <a class="reference external" href="http://benvanwerkhoven.github.io/kernel_tuner/user-api.html">interface
documentation</a>.</p>
<p>Let&#8217;s start with the basics. The Kernel Tuner has to know at least the
following things:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">kernel_name</span></code>: The name of the kernel</li>
<li><code class="docutils literal"><span class="pre">kernel_string</span></code>: The source code that contains that kernel</li>
<li><code class="docutils literal"><span class="pre">problem_size</span></code>: The domain over which you create threads and thread
blocks</li>
<li><code class="docutils literal"><span class="pre">arguments</span></code>: The arguments to use when calling the kernel</li>
<li><code class="docutils literal"><span class="pre">tune_params</span></code>: The dictionary with tunable parameters</li>
</ul>
<p>Note that, by executing the block at the top of this file we have
already loaded the CUDA kernel code as a string and stored it as
<code class="docutils literal"><span class="pre">kernel_string</span></code>.</p>
<p>We can use Numpy to generate some random input data, and create a list
of arguments that matches the argument list of our <code class="docutils literal"><span class="pre">stencil_kernel</span></code>
function written in CUDA. It is important that the order and type
matches the function specification of our kernel.</p>
<div class="nbinput nblast container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span>

<span class="n">problem_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">size</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">problem_size</span><span class="p">)</span>

<span class="n">x_old</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">x_new</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">x_old</span><span class="p">)</span>
<span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_new</span><span class="p">,</span> <span class="n">x_old</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>Note that <code class="docutils literal"><span class="pre">size</span></code> matches the size of the domain in used in the CUDA
code. Moreover, we use <code class="docutils literal"><span class="pre">astype</span></code> to ensure that the Numpy array
consists of 32-bit floating-point values, as expected by our CUDA
kernel.</p>
<p>Instead of generating random data you can of course also use data from a
file, Python offers many convenient functions for this, for example take
look at numpy.fromfile() or numpy.loadtxt().</p>
<p>The list named <code class="docutils literal"><span class="pre">args</span></code> will be used as the argument list that we&#8217;ll
pass to tune_kernel. The Kernel Tuner requires that list contains only
Numpy arrays or Numpy scalar values, and that the order of arguments
matches that of the CUDA kernel.</p>
<p>Now we are almost ready to call tune_kernel(). However, we have not
told the Kernel Tuner anything about how many thread blocks should be
created to launch the kernel. If you do not specify this the Kernel
Tuner will assume a default way for computing the number of thread
blocks. The grid dimension in the x-direction will default to
<code class="docutils literal"><span class="pre">problem_size[0]</span> <span class="pre">/</span> <span class="pre">block_size_x</span></code> and the y-direction will default to
<code class="docutils literal"><span class="pre">1</span></code>. This is convenient for small 1D kernels like vector add, but for
our 2D stencil kernel we need to specify how the tuning parameters
divide our problem size.</p>
<p>You can tell the Kernel Tuner how to determine the number of blocks it
should create through the so called grid divisor lists, which you can
specify using the optional arguments <code class="docutils literal"><span class="pre">grid_div_x</span></code> and <code class="docutils literal"><span class="pre">grid_div_y</span></code>.
Now let&#8217;s look at an example of how to setup these grid divisor lists.</p>
<p>So for our 2D stencil kernel, we have a 2D domain over which we want to
create threads and thread blocks in a way that we create one thread for
each element in the domain. So to get to the number of thread blocks,
the Kernel Tuner should just divide the problem_size in a particular
dimension with the thread block size in that dimension. Therefore, we
specify the following:</p>
<div class="nbinput nblast container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">grid_div_x</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;block_size_x&quot;</span><span class="p">]</span>
<span class="n">grid_div_y</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;block_size_y&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>Note that these are lists, you can add multiple tuning parameters to the
list. If you want to, you can even write arithmetic expressions within
these strings. The Kernel Tuner will evaluate all strings and multiply
them together. Then it will use this product to divide the problem size
in that dimension <strong>rounded up</strong>.</p>
</div>
<div class="section" id="Putting-it-all-together">
<h3>Putting it all together<a class="headerlink" href="#Putting-it-all-together" title="Permalink to this headline">Â¶</a></h3>
<p>Now let&#8217;s put everything that we&#8217;ve gone through together in a Python
script so you can try it out and see what it does.</p>
<div class="nbinput nblast container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">kernel_tuner</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">kernel_tuner</span><span class="o">.</span><span class="n">tune_kernel</span><span class="p">(</span><span class="s2">&quot;stencil_kernel&quot;</span><span class="p">,</span> <span class="n">kernel_string</span><span class="p">,</span> <span class="n">problem_size</span><span class="p">,</span>
        <span class="n">args</span><span class="p">,</span> <span class="n">tune_params</span><span class="p">,</span> <span class="n">grid_div_x</span><span class="o">=</span><span class="n">grid_div_x</span><span class="p">,</span> <span class="n">grid_div_y</span><span class="o">=</span><span class="n">grid_div_y</span><span class="p">,</span>
        <span class="n">verbose</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>When you call <code class="docutils literal"><span class="pre">tune_kernel()</span></code> the Kernel Tuner begins with printing
the name of the GPU it is using, just to be sure which GPU is used when
you have multiple GPUs in your system.</p>
<p>After that it will iterative compile and benchmark the kernel for every
possible combination of all values of all tunable parameters. As you
will see in the output, the Kernel Tuner will automatically skip over
kernels that use too many threads per block. It will do that too for
kernels that use too much shared memory or too many registers. These
configurations are by default skipped silently, but are currently being
printed because we have passed the <code class="docutils literal"><span class="pre">verbose=True</span></code> option.</p>
<p>As a convenience, the Kernel Tuner also prints the best performing
combination of tunable parameters. It is however, more useful to be able
to load the tuning results as data.</p>
</div>
<div class="section" id="Storing-the-results">
<h3>Storing the results<a class="headerlink" href="#Storing-the-results" title="Permalink to this headline">Â¶</a></h3>
<p>While it&#8217;s nice that the Kernel Tuner prints the results to stdout, it&#8217;s
not that great if we had to parse what is printed to get the results.
That is why the <code class="docutils literal"><span class="pre">tune_kernel()</span></code> returns a data structure that holds
all the results.</p>
<p><code class="docutils literal"><span class="pre">tune_kernel</span></code> returns a list of dictionaries, where each benchmarked
kernel is represented by a dictionary contain the tunable parameters for
that particular kernel configuration and one more entry called &#8216;time&#8217;.
The list of dictionaries format is very flexible and can easily be
converted to easy to parse formats like json or csv for further
analysis.</p>
<p>You can execute the following code block to store the tuning results to
both a json and a csv file.</p>
<div class="nbinput nblast container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1">#store output as json</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;tutorial.json&quot;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">fp</span><span class="p">)</span>

<span class="c1">#store output as csv</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="k">import</span> <span class="n">DataFrame</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;tutorial.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="convolution.html" class="btn btn-neutral float-right" title="Convolution Example" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="examples.html" class="btn btn-neutral" title="Kernel Tuner Examples" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, Ben van Werkhoven.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>