

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>API Documentation &mdash; Kernel Tuner 0.1.6 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="Kernel Tuner 0.1.6 documentation" href="index.html"/>
        <link rel="next" title="Design documentation" href="design.html"/>
        <link rel="prev" title="Tuning Host Code" href="hostcode.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Kernel Tuner
          

          
          </a>

          
            
            
              <div class="version">
                0.1.6
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="convolution.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="diffusion.html">Tutorial: From physics to tuned GPU kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Kernel Tuner Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix.html">Matrix Multiply Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="correctness.html">Kernel Correctness Verification</a></li>
<li class="toctree-l1"><a class="reference internal" href="hostcode.html">Tuning Host Code</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="design.html">Design documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contribution guide</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">Kernel Tuner</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>API Documentation</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="https://github.com/benvanwerkhoven/kernel_tuner/blob/master/doc/source/user-api.rst" class="fa fa-github"> Edit on GitHub</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="toctree-wrapper compound">
</div>
<div class="section" id="api-documentation">
<span id="details"></span><h1>API Documentation<a class="headerlink" href="#api-documentation" title="Permalink to this headline">¶</a></h1>
<p>This file provides all the details you need about how to call the Kernel Tuner&#8217;s functions, including all the optional arguments.</p>
<dl class="function">
<dt id="kernel_tuner.tune_kernel">
<code class="descclassname">kernel_tuner.</code><code class="descname">tune_kernel</code><span class="sig-paren">(</span><em>kernel_name</em>, <em>kernel_string</em>, <em>problem_size</em>, <em>arguments</em>, <em>tune_params</em>, <em>grid_div_x=None</em>, <em>grid_div_y=None</em>, <em>grid_div_z=None</em>, <em>restrictions=None</em>, <em>answer=None</em>, <em>atol=1e-06</em>, <em>verify=None</em>, <em>verbose=False</em>, <em>lang=None</em>, <em>device=0</em>, <em>platform=0</em>, <em>cmem_args=None</em>, <em>num_threads=1</em>, <em>use_noodles=False</em>, <em>sample_fraction=False</em>, <em>compiler_options=None</em>, <em>log=None</em>, <em>iterations=7</em>, <em>block_size_names=None</em>, <em>quiet=False</em>, <em>strategy=None</em>, <em>method=None</em><span class="sig-paren">)</span><a class="headerlink" href="#kernel_tuner.tune_kernel" title="Permalink to this definition">¶</a></dt>
<dd><p>Tune a CUDA kernel given a set of tunable parameters</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>kernel_name</strong> (<em>string</em>) &#8211; The name of the kernel in the code.</li>
<li><strong>kernel_string</strong> (<em>string or list and/or callable</em>) &#8211; <p>The CUDA, OpenCL, or C kernel code as a string.
It is also allowed for the string to be a filename of the file
containing the code.</p>
<p>To support combined host and device code tuning for runtime
compiled device code, a list of filenames can be passed instead.
The first file in the list should be the file that contains the
host code. The host code is allowed to include or read as a string
any of the files in the list beyond the first.</p>
<p>Another alternative is to pass a function instead, or instead
of the first item in the list of filenames. The purpose of this
is to support the use of code generating functions that generate
the kernel code based on the specific parameters. This function
should take one positional argument, which will be used to pass
a dict containing the parameters. The function should return a
string with the source code for the kernel.</p>
</li>
<li><strong>problem_size</strong> (<em>string, int, or tuple(int or string, ..)</em>) &#8211; <p>An int or string, or 1,2,3-dimensional tuple
containing the size from which the grid dimensions of the kernel
will be computed.</p>
<p>Do not divide the problem_size yourself by the thread block sizes.
The Kernel Tuner does this for you based on tunable parameters,
called &#8220;block_size_x&#8221;, &#8220;block_size_y&#8221;, and &#8220;block_size_z&#8221;.
If more or different parameters divide the grid dimensions use
grid_div_x/y/z options to specify this.</p>
<p>You are allowed to use a string to specify the problem
size. Within a string you are allowed to write Python
arithmetic and use the names of tunable parameters as variables
in these expressions.
The Kernel Tuner will replace instances of the tunable parameters
with their current value when computing the grid dimensions.
See the reduction CUDA example for an example use of this feature.</p>
</li>
<li><strong>arguments</strong> (<em>list</em>) &#8211; A list of kernel arguments, use numpy arrays for
arrays, use numpy.int32 or numpy.float32 for scalars.</li>
<li><strong>grid_div_x</strong> (<em>list</em>) &#8211; <p>A list of names of the parameters whose values divide
the grid dimensions in the x-direction.
The product of all grid divisor expressions is computed before dividing
the problem_size in that dimension. Also note that the divison is treated
as a float divison and resulting grid dimensions will be rounded up to
the nearest integer number.</p>
<p>Arithmetic expressions can be
used if necessary inside the string containing a parameter name. For
example, in some cases you may want to divide the problem size in the
x-dimension with the number of warps rather than the number of threads
in a block, in such cases one could use [&#8220;block_size_x/32&#8221;].</p>
<p>If not supplied, [&#8220;block_size_x&#8221;] will be used by default, if you do not
want any grid x-dimension divisors pass an empty list.</p>
</li>
<li><strong>grid_div_y</strong> (<em>list</em>) &#8211; A list of names of the parameters whose values divide
the grid dimensions in the y-direction, [&#8220;block_size_y&#8221;] by default.
If you do not want to divide the problem_size, you should pass an empty list.
See grid_div_x for more details.</li>
<li><strong>grid_div_z</strong> (<em>list</em>) &#8211; A list of names of the parameters whose values divide
the grid dimensions in the z-direction, [&#8220;block_size_z&#8221;] by default.
If you do not want to divide the problem_size, you should pass an empty list.
See grid_div_x for more details.</li>
<li><strong>cmem_args</strong> (<em>dict(string: numpy object)</em>) &#8211; CUDA-specific feature for specifying constant memory
arguments to the kernel. In OpenCL these are handled as normal
kernel arguments, but in CUDA you can copy to a symbol. The way you
specify constant memory arguments is by passing a dictionary with
strings containing the constant memory symbol name together with numpy
objects in the same way as normal kernel arguments.</li>
<li><strong>block_size_names</strong> (<em>list(string)</em>) &#8211; A list of strings that replace the defaults for the names
that denote the thread block dimensions. If not passed, the behavior
defaults to <code class="docutils literal"><span class="pre">[&quot;block_size_x&quot;,</span> <span class="pre">&quot;block_size_y&quot;,</span> <span class="pre">&quot;block_size_z&quot;]</span></code></li>
<li><strong>tune_params</strong> (<em>dict( string : [...]</em>) &#8211; <p>A dictionary containing the parameter names as keys,
and lists of possible parameter settings as values.
The Kernel Tuner will try to compile and benchmark all possible
combinations of all possible values for all tuning parameters.
This typically results in a rather large search space of all
possible kernel configurations.</p>
<p>For each kernel configuration, each tuning parameter is
replaced at compile-time with its current value.
Currently, the Kernel Tuner uses the convention that the following
list of tuning parameters are used as thread block dimensions:</p>
<blockquote>
<div><ul>
<li>&#8220;block_size_x&#8221;   thread block (work group) x-dimension</li>
<li>&#8220;block_size_y&#8221;   thread block (work group) y-dimension</li>
<li>&#8220;block_size_z&#8221;   thread block (work group) z-dimension</li>
</ul>
</div></blockquote>
<p>Options for changing these defaults may be added later. If you
don&#8217;t want the thread block dimensions to be compiled in, you
may use the built-in variables blockDim.xyz in CUDA or the
built-in function get_local_size() in OpenCL instead.</p>
</li>
<li><strong>restrictions</strong> (<em>list</em>) &#8211; A list of strings containing boolean expression that
limit the search space in that they must be satisfied by the kernel
configuration. These expressions must be true for the configuration
to be part of the search space. For example:
restrictions=[&#8220;block_size_x==block_size_y*tile_size_y&#8221;] limits the
search to configurations where the block_size_x equals the product
of block_size_y and tile_size_y.
The default is None.</li>
<li><strong>answer</strong> (<em>list</em>) &#8211; A list of arguments, similar to what you pass to arguments,
that contains the expected output of the kernel after it has executed
and contains None for each argument that is input-only. The expected
output of the kernel will then be used to verify the correctness of
each kernel in the parameter space before it will be benchmarked.</li>
<li><strong>atol</strong> (<em>float</em>) &#8211; The maximum allowed absolute difference between two elements
in the output and the reference answer, as passed to numpy.allclose().
Ignored if you have not passed a reference answer. Default value is
1e-6, that is 0.000001.</li>
<li><strong>verify</strong> (<em>func(ref, ans, atol=None)</em>) &#8211; <p>Python function used for output verification. By default,
numpy.allclose is used for output verification, if this does not suit
your application, you can pass a different function here.</p>
<p>The function is expected to have two positional arguments. The first
is the reference result, the second is the output computed by the
kernel being verified. The types of these arguments depends on the
type of the output arguments you are verifying. The function may also
have an optional argument named atol, to which the value will be
passed that was specified using the atol option to tune_kernel.
The function should return True when the output passes the test, and
False when the output fails the test.</p>
</li>
<li><strong>sample_fraction</strong> (<em>float</em>) &#8211; Benchmark only a sample fraction of the search space, False by
default. To enable sampling, pass a value between 0 and 1.</li>
<li><strong>use_noodles</strong> (<em>boolean</em>) &#8211; Use Noodles workflow engine to tune in parallel using
multiple threads, False by Default.
Requires Noodles to be installed, use &#8216;pip install noodles&#8217;.
Note that Noodles requires Python 3.5 or newer.
You can configure the number of threads to use with the option
num_threads.</li>
<li><strong>num_threads</strong> (<em>int</em>) &#8211; The number of threads to use when using the Noodles
workflow engine for tuning using multiple threads, 1 by default.
Requires Noodles, see &#8216;use_noodles&#8217; option.</li>
<li><strong>strategy</strong> &#8211; <p>Specify the strategy to use for searching through the
parameter space, choose from:</p>
<blockquote>
<div><ul>
<li>&#8220;brute_force&#8221; (default),</li>
<li>&#8220;random_sample&#8221;, specify: <em>sample_fraction</em>,</li>
<li>&#8220;minimize&#8221; or &#8220;basinhopping&#8221;, specify: <em>method</em>,</li>
<li>&#8220;diff_evo&#8221;, specify: <em>method</em>.</li>
</ul>
</div></blockquote>
<p>&#8220;brute_force&#8221; is the default and iterates over the entire search
space.</p>
<p>&#8220;random_sample&#8221; can be used to only benchmark a fraction of the
search space, specify a <em>sample_fraction</em> in the interval [0, 1].</p>
<p>&#8220;minimize&#8221; and &#8220;basinhopping&#8221; strategies use minimizers to
limit the search through the parameter space, select any of the
methods: &#8220;Nelder-Mead&#8221;, &#8220;Powell&#8221;, &#8220;CG&#8221;, &#8220;BFGS&#8221;, &#8220;L-BFGS-B&#8221;,
&#8220;TNC&#8221;, &#8220;COBYLA&#8221;, or &#8220;SLSQP&#8221;. It is also possible to pass a
function that implements a custom minimization strategy.</p>
<p>&#8220;diff_evo&#8221; uses differential evolution and supports the following
evolution strategies, which can be passed using the <em>method</em> argument:
&#8220;best1bin&#8221;, &#8220;best1exp&#8221;, &#8220;rand1exp&#8221;, &#8220;randtobest1exp&#8221;, &#8220;best2exp&#8221;,
&#8220;rand2exp&#8221;, &#8220;randtobest1bin&#8221;, &#8220;best2bin&#8221;, &#8220;rand2bin&#8221;, &#8220;rand1bin&#8221;.
The default is &#8220;best1bin&#8221;.</p>
</li>
<li><strong>method</strong> (<em>string or callable</em>) &#8211; <p>Specify a method for the strategy that searches through
the parameter space during tuning.</p>
<p>When using strategy=&#8221;minimize&#8221; or strategy=&#8221;basinhopping&#8221;, the
following options are supported:
&#8220;Nelder-Mead&#8221;, &#8220;Powell&#8221;, &#8220;CG&#8221;, &#8220;BFGS&#8221;, &#8220;L-BFGS-B&#8221;,
&#8220;TNC&#8221;, &#8220;COBYLA&#8221;, or &#8220;SLSQP&#8221;. It is also possible to pass a function
that implements a custom minimization strategy.</p>
<p>When using strategy=&#8221;diff_evo&#8221;, the following options are supported:
&#8220;best1bin&#8221;, &#8220;best1exp&#8221;, &#8220;rand1exp&#8221;, &#8220;randtobest1exp&#8221;, &#8220;best2exp&#8221;,
&#8220;rand2exp&#8221;, &#8220;randtobest1bin&#8221;, &#8220;best2bin&#8221;, &#8220;rand2bin&#8221;, &#8220;rand1bin&#8221;.</p>
</li>
<li><strong>iterations</strong> (<em>int</em>) &#8211; The number of times a kernel should be executed and
its execution time measured when benchmarking a kernel, 7 by default.</li>
<li><strong>verbose</strong> (<em>bool</em>) &#8211; <p>Sets whether or not to report about configurations that
were skipped during the search. This could be due to several reasons:</p>
<blockquote>
<div><ul>
<li>kernel configuration fails one or more restrictions</li>
<li>too many threads per thread block</li>
<li>too much shared memory used by the kernel</li>
<li>too many resources requested for launch</li>
</ul>
</div></blockquote>
<p>verbose is False by default.</p>
</li>
<li><strong>lang</strong> (<em>string</em>) &#8211; Specifies the language used for GPU kernels. The kernel_tuner
automatically detects the language, but if it fails, you may specify
the language using this argument, currently supported: &#8220;CUDA&#8221;,
&#8220;OpenCL&#8221;, or &#8220;C&#8221;.</li>
<li><strong>device</strong> (<em>int</em>) &#8211; CUDA/OpenCL device to use, in case you have multiple
CUDA-capable GPUs or OpenCL devices you may use this to select one,
0 by default. Ignored if you are tuning host code by passing
lang=&#8221;C&#8221;.</li>
<li><strong>platform</strong> (<em>int</em>) &#8211; OpenCL platform to use, in case you have multiple
OpenCL platforms you may use this to select one,
0 by default. Ignored if not using OpenCL.</li>
<li><strong>quiet</strong> (<em>boolean</em>) &#8211; Control whether or not to print to the console which
device is being used, False by default</li>
<li><strong>compiler_options</strong> (<em>list(string)</em>) &#8211; A list of strings that specify compiler
options.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A list of dictionaries of all executed kernel configurations and their
execution times. And a dictionary with information about the environment
in which the tuning took place. This records device name, properties,
version info, and so on.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list(dict()), dict()</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="kernel_tuner.run_kernel">
<code class="descclassname">kernel_tuner.</code><code class="descname">run_kernel</code><span class="sig-paren">(</span><em>kernel_name</em>, <em>kernel_string</em>, <em>problem_size</em>, <em>arguments</em>, <em>params</em>, <em>grid_div_x=None</em>, <em>grid_div_y=None</em>, <em>grid_div_z=None</em>, <em>lang=None</em>, <em>device=0</em>, <em>platform=0</em>, <em>cmem_args=None</em>, <em>compiler_options=None</em>, <em>block_size_names=None</em>, <em>quiet=False</em><span class="sig-paren">)</span><a class="headerlink" href="#kernel_tuner.run_kernel" title="Permalink to this definition">¶</a></dt>
<dd><p>Compile and run a single kernel</p>
<p>Compiles and runs a single kernel once, given a specific instance of the kernels tuning parameters.
However, instead of measuring execution time run_kernel returns the output of the kernel.
The output is returned as a list of numpy arrays that contains the state of all the kernel arguments
after execution on the GPU.</p>
<dl class="docutils">
<dt>To summarize what this function will do for you in one call:</dt>
<dd><ul class="first last simple">
<li>Compile the kernel according to the set of parameters passed</li>
<li>Allocate GPU memory to hold all kernel arguments</li>
<li>Move the all data to the GPU</li>
<li>Execute the kernel on the GPU</li>
<li>Copy all data from the GPU back to the host and return it as a list of Numpy arrays</li>
</ul>
</dd>
</dl>
<p>This function was added to the Kernel Tuner mostly to allow easy testing for kernel correctness.
On purpose, the interface is a lot like <cite>tune_kernel()</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>kernel_name</strong> (<em>string</em>) &#8211; The name of the kernel in the code.</li>
<li><strong>kernel_string</strong> (<em>string or list and/or callable</em>) &#8211; <p>The CUDA, OpenCL, or C kernel code as a string.
It is also allowed for the string to be a filename of the file
containing the code.</p>
<p>To support combined host and device code tuning for runtime
compiled device code, a list of filenames can be passed instead.
The first file in the list should be the file that contains the
host code. The host code is allowed to include or read as a string
any of the files in the list beyond the first.</p>
<p>Another alternative is to pass a function instead, or instead
of the first item in the list of filenames. The purpose of this
is to support the use of code generating functions that generate
the kernel code based on the specific parameters. This function
should take one positional argument, which will be used to pass
a dict containing the parameters. The function should return a
string with the source code for the kernel.</p>
</li>
<li><strong>problem_size</strong> (<em>string, int, or tuple(int or string, ..)</em>) &#8211; <p>An int or string, or 1,2,3-dimensional tuple
containing the size from which the grid dimensions of the kernel
will be computed.</p>
<p>Do not divide the problem_size yourself by the thread block sizes.
The Kernel Tuner does this for you based on tunable parameters,
called &#8220;block_size_x&#8221;, &#8220;block_size_y&#8221;, and &#8220;block_size_z&#8221;.
If more or different parameters divide the grid dimensions use
grid_div_x/y/z options to specify this.</p>
<p>You are allowed to use a string to specify the problem
size. Within a string you are allowed to write Python
arithmetic and use the names of tunable parameters as variables
in these expressions.
The Kernel Tuner will replace instances of the tunable parameters
with their current value when computing the grid dimensions.
See the reduction CUDA example for an example use of this feature.</p>
</li>
<li><strong>arguments</strong> (<em>list</em>) &#8211; A list of kernel arguments, use numpy arrays for
arrays, use numpy.int32 or numpy.float32 for scalars.</li>
<li><strong>grid_div_x</strong> (<em>list</em>) &#8211; <p>A list of names of the parameters whose values divide
the grid dimensions in the x-direction.
The product of all grid divisor expressions is computed before dividing
the problem_size in that dimension. Also note that the divison is treated
as a float divison and resulting grid dimensions will be rounded up to
the nearest integer number.</p>
<p>Arithmetic expressions can be
used if necessary inside the string containing a parameter name. For
example, in some cases you may want to divide the problem size in the
x-dimension with the number of warps rather than the number of threads
in a block, in such cases one could use [&#8220;block_size_x/32&#8221;].</p>
<p>If not supplied, [&#8220;block_size_x&#8221;] will be used by default, if you do not
want any grid x-dimension divisors pass an empty list.</p>
</li>
<li><strong>grid_div_y</strong> (<em>list</em>) &#8211; A list of names of the parameters whose values divide
the grid dimensions in the y-direction, [&#8220;block_size_y&#8221;] by default.
If you do not want to divide the problem_size, you should pass an empty list.
See grid_div_x for more details.</li>
<li><strong>grid_div_z</strong> (<em>list</em>) &#8211; A list of names of the parameters whose values divide
the grid dimensions in the z-direction, [&#8220;block_size_z&#8221;] by default.
If you do not want to divide the problem_size, you should pass an empty list.
See grid_div_x for more details.</li>
<li><strong>cmem_args</strong> (<em>dict(string: numpy object)</em>) &#8211; CUDA-specific feature for specifying constant memory
arguments to the kernel. In OpenCL these are handled as normal
kernel arguments, but in CUDA you can copy to a symbol. The way you
specify constant memory arguments is by passing a dictionary with
strings containing the constant memory symbol name together with numpy
objects in the same way as normal kernel arguments.</li>
<li><strong>block_size_names</strong> (<em>list(string)</em>) &#8211; A list of strings that replace the defaults for the names
that denote the thread block dimensions. If not passed, the behavior
defaults to <code class="docutils literal"><span class="pre">[&quot;block_size_x&quot;,</span> <span class="pre">&quot;block_size_y&quot;,</span> <span class="pre">&quot;block_size_z&quot;]</span></code></li>
<li><strong>params</strong> (<em>dict( string: int )</em>) &#8211; A dictionary containing the tuning parameter names as keys
and a single value per tuning parameter as values.</li>
<li><strong>lang</strong> (<em>string</em>) &#8211; Specifies the language used for GPU kernels. The kernel_tuner
automatically detects the language, but if it fails, you may specify
the language using this argument, currently supported: &#8220;CUDA&#8221;,
&#8220;OpenCL&#8221;, or &#8220;C&#8221;.</li>
<li><strong>device</strong> (<em>int</em>) &#8211; CUDA/OpenCL device to use, in case you have multiple
CUDA-capable GPUs or OpenCL devices you may use this to select one,
0 by default. Ignored if you are tuning host code by passing
lang=&#8221;C&#8221;.</li>
<li><strong>platform</strong> (<em>int</em>) &#8211; OpenCL platform to use, in case you have multiple
OpenCL platforms you may use this to select one,
0 by default. Ignored if not using OpenCL.</li>
<li><strong>quiet</strong> (<em>boolean</em>) &#8211; Control whether or not to print to the console which
device is being used, False by default</li>
<li><strong>compiler_options</strong> (<em>list(string)</em>) &#8211; A list of strings that specify compiler
options.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A list of numpy arrays, similar to the arguments passed to this
function, containing the output after kernel execution.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="design.html" class="btn btn-neutral float-right" title="Design documentation" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="hostcode.html" class="btn btn-neutral" title="Tuning Host Code" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, Ben van Werkhoven.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1.6',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>